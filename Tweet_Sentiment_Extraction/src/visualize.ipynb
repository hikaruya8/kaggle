{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_tweets_and_sentiment_label_loaders\n",
    "from model import TransformerClassification\n",
    "import torch\n",
    "from IPython.display import HTML\n",
    "\n",
    "def highlight(word, attn):\n",
    "    # Attentionの値が大きければ文字の背景が濃い赤になるHTMLを出力させる関数\n",
    "    # import pdb;pdb.set_trace()\n",
    "    html_color = '#%02X%02X%02X' % (\n",
    "        255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
    "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)\n",
    "\n",
    "def mk_html(index, batch, preds, normalized_weights_1, normalized_weights_2, TEST_TEXT):\n",
    "    # HTMLデータの作成\n",
    "\n",
    "    # index の結果抽出\n",
    "    sentence = batch.Test_Text[0][index] #文章\n",
    "    label = batch.Test_Label[index]\n",
    "    pred = preds[index]\n",
    "\n",
    "    # indexのattentionを抽出、規格化\n",
    "    attens1 = normalized_weights_1[index, 0, :] # 0番目 <cls>のAttention\n",
    "    attens1 /= attens1.max()\n",
    "    attens2 = normalized_weights_2[index, 0, :] # 0番目 <cls>のAttention\n",
    "    attens2 /= attens2.max()\n",
    "\n",
    "    # ラベルの予測結果を文字に置き換え\n",
    "    if label == 0:\n",
    "        label_str = 'Neutral'\n",
    "    elif label == 1:\n",
    "        label_str = 'Positive'\n",
    "    else:\n",
    "        label_str = 'Negative'\n",
    "\n",
    "    if pred == 0:\n",
    "        pred_str = 'Neutral'\n",
    "    elif label == 1:\n",
    "        pred_str = 'Positive'\n",
    "    else:\n",
    "        pred_str = 'Negative'\n",
    "\n",
    "    # 表示用のHTMLを作成\n",
    "    html = '正解ラベル: {}<br>推論ラベル: {}<br><br>'.format(label_str, pred_str)\n",
    "\n",
    "    # １段目のAttention\n",
    "    html += '[TransformerBlockの１段目のAttentionを可視化]<br>'\n",
    "    for word, attn, in zip(sentence, attens1):\n",
    "        html += highlight(TEST_TEXT.vocab.itos[word], attn)\n",
    "    html += \"<br><br>\"\n",
    "\n",
    "    # 2段目のAttention\n",
    "    html += '[TransformerBlockの2段目のAttentionを可視化]<br>'\n",
    "    for word, attn, in zip(sentence, attens2):\n",
    "        html += highlight(TEST_TEXT.vocab.itos[word], attn)\n",
    "\n",
    "    html += \"<br><br>\"\n",
    "\n",
    "    return html\n",
    "\n",
    "# modelの保存場所\n",
    "saved_model_path = '../model/transformer.pth' \n",
    "# データの読み込み\n",
    "train_dl, val_dl, test_dl, TEXT1, TEXT2, TEST_TEXT = get_tweets_and_sentiment_label_loaders(max_length=256, batch_size=64)\n",
    "# load model\n",
    "net_trained = TransformerClassification(text_embedding_vectors=TEXT1.vocab.vectors, d_model=300, max_seq_len=256, output_dim=3)\n",
    "net_trained.load_state_dict(torch.load(saved_model_path))\n",
    "\n",
    "batch = next(iter(test_dl))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs = batch.Test_Text[0].to(device)\n",
    "labels = batch.Test_Label.to(device)\n",
    "\n",
    "#make mask\n",
    "input_pad = 1\n",
    "input_mask = (inputs != input_pad)\n",
    "\n",
    "\n",
    "outputs, normalized_weights_1, normalized_weights_2 = net_trained(inputs, input_mask)\n",
    "_, preds = torch.max(outputs, 1)\n",
    "\n",
    "index = 22 # 出力させたいデータ\n",
    "html_output = mk_html(index, batch, preds, normalized_weights_1, normalized_weights_2, TEST_TEXT)\n",
    "HTML(html_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
